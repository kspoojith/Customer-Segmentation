{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d3f73b",
   "metadata": {},
   "source": [
    "# Customer Segmentation for E-commerce Personalization\n",
    "\n",
    "This notebook implements a complete customer segmentation workflow to support personalized marketing. It covers data loading, EDA, preprocessing, feature engineering (including RFM/behavioral proxies), clustering experiments (KMeans, Agglomerative, DBSCAN), evaluation, visualization, and export of segment definitions and artifacts.\n",
    "\n",
    "Objectives:\n",
    "- Build and evaluate clustering models to discover meaningful customer segments.\n",
    "- Describe segments and generate targeted marketing recommendations.\n",
    "- Produce reproducible artifacts (pipeline + model + labeled dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42022062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 - Imports and environment check\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "# Data science libs (optional if not installed)\n",
    "import importlib\n",
    "\n",
    "REQUIRED = [\n",
    "    'pandas', 'numpy', 'sklearn', 'matplotlib', 'seaborn', 'joblib'\n",
    "]\n",
    "\n",
    "print('Python', sys.version)\n",
    "print('Platform', platform.platform())\n",
    "\n",
    "for pkg in REQUIRED:\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        print(pkg, 'version', getattr(m, '__version__', 'unknown'))\n",
    "    except Exception as e:\n",
    "        print(pkg, 'NOT INSTALLED - you can install with pip')\n",
    "\n",
    "# reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# plotting defaults\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 - Load dataset (data.csv)\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'data.csv')\n",
    "print('Looking for data at', DATA_PATH)\n",
    "\n",
    "# Read with pandas, try to handle whitespace header quirks\n",
    "raw = pd.read_csv(DATA_PATH, encoding='utf-8', engine='python')\n",
    "# Clean column names\n",
    "raw.columns = [c.strip() for c in raw.columns]\n",
    "\n",
    "print('Shape:', raw.shape)\n",
    "raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 - Quick data audit and EDA\n",
    "\n",
    "# Basic info\n",
    "print('Columns and dtypes:')\n",
    "print(raw.dtypes)\n",
    "\n",
    "print('\\nMissing values per column:')\n",
    "print(raw.isnull().sum())\n",
    "\n",
    "# Basic numeric describe\n",
    "num_cols = raw.select_dtypes(include=['number']).columns.tolist()\n",
    "print('\\nNumeric columns:', num_cols)\n",
    "\n",
    "if len(num_cols) > 0:\n",
    "    display(raw[num_cols].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]).T)\n",
    "\n",
    "# Visual quick checks: histograms for numeric columns\n",
    "import matplotlib.pyplot as plt\n",
    "for c in num_cols:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.histplot(raw[c].dropna(), kde=True)\n",
    "    plt.title(c)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c85acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 - Data cleaning (missing values, types)\n",
    "\n",
    "# Example cleaning strategies (customize as you inspect data):\n",
    "# - Trim whitespace in string columns\n",
    "# - Replace obvious placeholders (e.g., empty strings) with NaN\n",
    "# - Decide to drop columns that are not useful (Address may be dropped)\n",
    "\n",
    "str_cols = raw.select_dtypes(include=['object']).columns.tolist()\n",
    "for c in str_cols:\n",
    "    raw[c] = raw[c].astype(str).str.strip()\n",
    "\n",
    "# Example: drop Address column (often not relevant for segmentation)\n",
    "if 'Address' in raw.columns:\n",
    "    print('Dropping Address column to focus on behavioral features')\n",
    "    raw = raw.drop(columns=['Address'])\n",
    "\n",
    "# Quick check after cleaning\n",
    "raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5 - Outlier detection and handling\n",
    "from scipy import stats\n",
    "\n",
    "# IQR-based outlier detection example for numeric columns\n",
    "def detect_outliers_iqr(df, cols):\n",
    "    outlier_idx = set()\n",
    "    for c in cols:\n",
    "        Q1 = df[c].quantile(0.25)\n",
    "        Q3 = df[c].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        idx = df[(df[c] < lower) | (df[c] > upper)].index\n",
    "        outlier_idx.update(idx.tolist())\n",
    "    return sorted(list(outlier_idx))\n",
    "\n",
    "numeric_cols = raw.select_dtypes(include=['number']).columns.tolist()\n",
    "outliers = detect_outliers_iqr(raw, numeric_cols)\n",
    "print('Detected', len(outliers), 'outlier rows (IQR method)')\n",
    "\n",
    "# Strategy options (choose one):\n",
    "# 1. Clip values to percentile bounds\n",
    "# # raw[numeric_cols] = raw[numeric_cols].clip(lower=raw[numeric_cols].quantile(0.01), upper=raw[numeric_cols].quantile(0.99), axis=1)\n",
    "# 2. Winsorize using scipy or manual clipping\n",
    "# 3. Remove outlier rows before clustering (careful - loses data)\n",
    "\n",
    "# Example: show boxplot before/after for a chosen column\n",
    "if len(numeric_cols) > 0:\n",
    "    c = numeric_cols[0]\n",
    "    plt.figure(figsize=(8,3))\n",
    "    sns.boxplot(x=raw[c])\n",
    "    plt.title(f'Boxplot - {c}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b608f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6 - Feature engineering (RFM-like & behavioral proxies)\n",
    "# Note: original dataset lacks transaction-level dates; we will use available proxies:\n",
    "# - Length of Membership (proxy for tenure)\n",
    "# - Time on App, Time on Website (engagement)\n",
    "# - Yearly Amount Spent (monetary)\n",
    "\n",
    "feat_df = raw.copy()\n",
    "\n",
    "# Convert numeric columns if needed\n",
    "for c in ['Time on App', 'Time on Website', 'Length of Membership', 'Yearly Amount Spent']:\n",
    "    if c in feat_df.columns:\n",
    "        feat_df[c] = pd.to_numeric(feat_df[c], errors='coerce')\n",
    "\n",
    "# Example derived features\n",
    "if 'Time on App' in feat_df.columns and 'Time on Website' in feat_df.columns:\n",
    "    feat_df['App_vs_Web_ratio'] = feat_df['Time on App'] / (feat_df['Time on Website'] + 1e-6)\n",
    "\n",
    "# Example: normalize yearly spend by membership length as avg annual spend per membership-year\n",
    "if 'Yearly Amount Spent' in feat_df.columns and 'Length of Membership' in feat_df.columns:\n",
    "    feat_df['Spend_per_membership_year'] = feat_df['Yearly Amount Spent'] / (feat_df['Length of Membership'] + 1e-6)\n",
    "\n",
    "# Select features for clustering\n",
    "candidate_features = [c for c in feat_df.columns if c in ['Time on App','Time on Website','Length of Membership','Yearly Amount Spent','App_vs_Web_ratio','Spend_per_membership_year']]\n",
    "print('Candidate features:', candidate_features)\n",
    "\n",
    "X = feat_df[candidate_features].copy()\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9de58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7 - Encoding categorical features\n",
    "# Avatar may be a color string; low-cardinality frequency encoding is often useful\n",
    "if 'Avatar' in feat_df.columns:\n",
    "    freq = feat_df['Avatar'].value_counts(normalize=True)\n",
    "    feat_df['Avatar_freq'] = feat_df['Avatar'].map(freq)\n",
    "    candidate_features.append('Avatar_freq')\n",
    "\n",
    "# If there are other categorical columns, consider one-hot or target encoding\n",
    "print('Updated candidate features:', candidate_features)\n",
    "X = feat_df[candidate_features].copy()\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8147105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8 - Feature scaling and pipeline construction\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# Build a simple pipeline for numeric features\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "X_prepared = numeric_pipeline.fit_transform(X)\n",
    "print('Prepared feature matrix shape:', X_prepared.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f134d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9 - Dimensionality reduction (PCA + UMAP)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.9, random_state=RANDOM_STATE)  # keep 90% variance\n",
    "X_pca = pca.fit_transform(X_prepared)\n",
    "print('PCA components retained:', pca.n_components_)\n",
    "\n",
    "# For visualization get 2D embedding via PCA and optionally UMAP\n",
    "import numpy as np\n",
    "X_pca_2d = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(X_prepared)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_pca_2d[:,0], X_pca_2d[:,1], s=20, alpha=0.6)\n",
    "plt.title('2D PCA projection of customers (unclustered)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "# UMAP (optional) - requires umap-learn\n",
    "try:\n",
    "    import umap\n",
    "    reducer = umap.UMAP(random_state=RANDOM_STATE)\n",
    "    X_umap = reducer.fit_transform(X_prepared)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X_umap[:,0], X_umap[:,1], s=20, alpha=0.6)\n",
    "    plt.title('UMAP 2D embedding (unclustered)')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('UMAP not available:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10 - Baseline clustering with KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "results = []\n",
    "K_RANGE = range(2,11)\n",
    "for k in K_RANGE:\n",
    "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = km.fit_predict(X_prepared)\n",
    "    sil = silhouette_score(X_prepared, labels) if len(set(labels))>1 else float('nan')\n",
    "    db = davies_bouldin_score(X_prepared, labels) if len(set(labels))>1 else float('nan')\n",
    "    ch = calinski_harabasz_score(X_prepared, labels) if len(set(labels))>1 else float('nan')\n",
    "    results.append({'k':k, 'silhouette':sil, 'davies_bouldin':db, 'calinski_harabasz':ch})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n",
    "\n",
    "# Plot silhouette vs k\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(results_df['k'], results_df['silhouette'], marker='o')\n",
    "plt.xlabel('k (clusters)')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('KMeans silhouette vs k')\n",
    "plt.show()\n",
    "\n",
    "# Choose best k by silhouette\n",
    "best_k = int(results_df.loc[results_df['silhouette'].idxmax(), 'k'])\n",
    "print('Best k by silhouette:', best_k)\n",
    "\n",
    "best_km = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10).fit(X_prepared)\n",
    "labels_best = best_km.labels_\n",
    "feat_df['cluster_kmeans'] = labels_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11 - Alternative clustering: GMM, Agglomerative, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "\n",
    "# Gaussian Mixture\n",
    "gmm = GaussianMixture(n_components=best_k, random_state=RANDOM_STATE)\n",
    "gmm_labels = gmm.fit_predict(X_prepared)\n",
    "print('GMM silhouette', silhouette_score(X_prepared, gmm_labels))\n",
    "\n",
    "# Agglomerative\n",
    "agg = AgglomerativeClustering(n_clusters=best_k)\n",
    "agg_labels = agg.fit_predict(X_prepared)\n",
    "print('Agg silhouette', silhouette_score(X_prepared, agg_labels))\n",
    "\n",
    "# DBSCAN (baseline parameters)\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
    "db_labels = dbscan.fit_predict(X_prepared)\n",
    "unique_db = len(set(db_labels) - {-1})\n",
    "print('DBSCAN clusters (excluding noise):', unique_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12 - Cluster selection & evaluation\n",
    "# Consolidate metrics for chosen algorithms\n",
    "from collections import OrderedDict\n",
    "\n",
    "cluster_eval = OrderedDict()\n",
    "cluster_eval['kmeans'] = {\n",
    "    'labels': feat_df['cluster_kmeans'],\n",
    "    'silhouette': silhouette_score(X_prepared, feat_df['cluster_kmeans'])\n",
    "}\n",
    "cluster_eval['gmm'] = {'labels': gmm_labels, 'silhouette': silhouette_score(X_prepared, gmm_labels)}\n",
    "cluster_eval['agglomerative'] = {'labels': agg_labels, 'silhouette': silhouette_score(X_prepared, agg_labels)}\n",
    "\n",
    "for name, vals in cluster_eval.items():\n",
    "    print(name, 'silhouette:', vals['silhouette'])\n",
    "\n",
    "# If ground-truth labels exist (rare), compute ARI/NMI here\n",
    "# from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13 - Cluster profiling and top-3 characteristics per segment\n",
    "\n",
    "def cluster_profile(df, features, label_col='cluster_kmeans'):\n",
    "    profiles = {}\n",
    "    global_means = df[features].mean()\n",
    "    for label in sorted(df[label_col].unique()):\n",
    "        member = df[df[label_col]==label]\n",
    "        means = member[features].mean()\n",
    "        diff = (means - global_means) / (global_means.replace(0, 1))\n",
    "        top3 = diff.abs().sort_values(ascending=False).head(3).index.tolist()\n",
    "        profiles[label] = {\n",
    "            'size': len(member),\n",
    "            'means': means.to_dict(),\n",
    "            'top3_features': top3\n",
    "        }\n",
    "    return profiles\n",
    "\n",
    "features = candidate_features\n",
    "profiles = cluster_profile(feat_df, features, label_col='cluster_kmeans')\n",
    "\n",
    "import json\n",
    "print(json.dumps(profiles, indent=2)[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 14 - Segment visualization (scatter, heatmap, radar)\n",
    "# 2D scatter using PCA/UMAP colored by cluster\n",
    "if 'cluster_kmeans' in feat_df.columns:\n",
    "    lbls = feat_df['cluster_kmeans']\n",
    "    plt.figure(figsize=(7,5))\n",
    "    scatter = plt.scatter(X_pca_2d[:,0], X_pca_2d[:,1], c=lbls, cmap='tab10', s=20)\n",
    "    plt.legend(*scatter.legend_elements(), title='cluster')\n",
    "    plt.title('PCA 2D colored by KMeans cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Cluster centroid heatmap\n",
    "centroids = feat_df.groupby('cluster_kmeans')[features].mean()\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap((centroids - centroids.mean())/centroids.std(), annot=True, cmap='coolwarm')\n",
    "plt.title('Cluster centroids (standardized)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5accd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 15 - Personalized marketing rules and recommendations per segment\n",
    "# Example rule generation based on top features\n",
    "marketing_rules = {}\n",
    "for label, p in profiles.items():\n",
    "    top = p['top3_features']\n",
    "    size = p['size']\n",
    "    rule = {\n",
    "        'segment': int(label),\n",
    "        'size': int(size),\n",
    "        'top_features': top,\n",
    "        'recommended_action': 'Use targeted email campaigns focusing on product bundles for high spenders' if 'Yearly Amount Spent' in top else 'Promote app features for high app usage users'\n",
    "    }\n",
    "    marketing_rules[label] = rule\n",
    "\n",
    "import json\n",
    "print('Example marketing rules for segments:')\n",
    "print(json.dumps(marketing_rules, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c86960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 16 - Save results, export labeled dataset, and reproducibility\n",
    "import joblib\n",
    "\n",
    "# Save labeled dataset\n",
    "out_path = os.path.join(os.path.dirname(DATA_PATH), 'customer_segments.csv')\n",
    "feat_df.to_csv(out_path, index=False)\n",
    "print('Saved labeled customers to', out_path)\n",
    "\n",
    "# Save preprocessing pipeline and clustering model\n",
    "joblib.dump(numeric_pipeline, os.path.join(os.path.dirname(DATA_PATH), 'preprocessing_pipeline.joblib'))\n",
    "joblib.dump(best_km, os.path.join(os.path.dirname(DATA_PATH), 'kmeans_model.joblib'))\n",
    "print('Saved pipeline and model artifacts')\n",
    "\n",
    "# Save small manifest\n",
    "manifest = {\n",
    "    'created_at': datetime.utcnow().isoformat(),\n",
    "    'rows': int(feat_df.shape[0]),\n",
    "    'features': candidate_features,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "import json\n",
    "with open(os.path.join(os.path.dirname(DATA_PATH),'manifest.json'),'w',encoding='utf-8') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print('Wrote manifest.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 17 - Automated checks and lightweight tests\n",
    "# Minimal checks (not full pytest) to ensure preprocessing output shape and no NaNs\n",
    "print('Prepared shape:', getattr(X_prepared, 'shape', None))\n",
    "assert X_prepared is not None, 'X_prepared missing'\n",
    "import numpy as np\n",
    "assert not np.isnan(X_prepared).any(), 'NaNs present after preprocessing - review imputation'\n",
    "\n",
    "print('Smoke checks passed. If you want, I can add pytest-based tests in tests/test_preprocessing.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 15 (expanded) - Personalized marketing rules, uplift simulation, and export\n",
    "# We already created simple marketing_rules above. We'll expand with a light simulation and save recommendations.\n",
    "\n",
    "# Example: assume baseline conversion rate per cluster (these should come from real analytics; placeholders here)\n",
    "# If you have a real `conversion_rate` column, replace this with actual figures.\n",
    "baseline_conversion = {}\n",
    "for label, p in profiles.items():\n",
    "    # default placeholder: lower-spend clusters may have 1-2% baseline, mid-spend 2-4%, high-spend 4-8%\n",
    "    size = p['size']\n",
    "    # heuristic based on mean Yearly Amount Spent if available\n",
    "    mean_spend = p['means'].get('Yearly Amount Spent', 0)\n",
    "    if mean_spend >= 600:\n",
    "        baseline_conversion[label] = 0.06\n",
    "    elif mean_spend >= 450:\n",
    "        baseline_conversion[label] = 0.035\n",
    "    elif mean_spend >= 300:\n",
    "        baseline_conversion[label] = 0.02\n",
    "    else:\n",
    "        baseline_conversion[label] = 0.01\n",
    "\n",
    "# Define recommended action per segment more explicitly\n",
    "marketing_recommendations = {}\n",
    "for label, p in profiles.items():\n",
    "    top = p['top3_features']\n",
    "    rec = {}\n",
    "    # Channel and message tone\n",
    "    if 'Time on App' in top or 'App_vs_Web_ratio' in top:\n",
    "        rec['channel'] = 'Push notifications + in-app banners'\n",
    "        rec['message'] = 'Highlight app-exclusive deals and frictionless checkout'\n",
    "    elif 'Time on Website' in top:\n",
    "        rec['channel'] = 'Email + website personalization'\n",
    "        rec['message'] = 'Show tailored product carousels and limited-time discounts'\n",
    "    elif 'Yearly Amount Spent' in top or 'Spend_per_membership_year' in top:\n",
    "        rec['channel'] = 'Personalized email + premium offers'\n",
    "        rec['message'] = 'VIP bundles, loyalty offers, early access'\n",
    "    else:\n",
    "        rec['channel'] = 'Email + social ads'\n",
    "        rec['message'] = 'Promotional discounts and product discovery content'\n",
    "\n",
    "    # Offer type\n",
    "    if p['means'].get('Yearly Amount Spent', 0) >= 600:\n",
    "        rec['offer'] = 'Premium bundles, cross-sell high-margin items, loyalty program invite'\n",
    "    elif p['means'].get('Yearly Amount Spent', 0) >= 400:\n",
    "        rec['offer'] = 'Sitewide % discount or free shipping threshold'\n",
    "    else:\n",
    "        rec['offer'] = 'Intro discount (10-15%) and product recommendations'\n",
    "\n",
    "    # Expected uplift (conservative estimate). These are placeholders for planning A/B tests.\n",
    "    # We'll compute expected conversion after targeted campaign as baseline * (1 + uplift_pct)\n",
    "    uplift_pct = 0.15  # 15% uplift target; per-cluster tuning may change this\n",
    "    rec['baseline_conversion'] = baseline_conversion[label]\n",
    "    rec['expected_conversion'] = rec['baseline_conversion'] * (1 + uplift_pct)\n",
    "    rec['expected_absolute_lift'] = rec['expected_conversion'] - rec['baseline_conversion']\n",
    "    rec['size'] = p['size']\n",
    "    rec['top_features'] = top\n",
    "    rec['recommended_action'] = marketing_rules.get(label, {}).get('recommended_action', '')\n",
    "\n",
    "    marketing_recommendations[label] = rec\n",
    "\n",
    "# Save recommendations and a short human-readable report\n",
    "import json\n",
    "rec_path = os.path.join(os.path.dirname(DATA_PATH), 'marketing_recommendations.json')\n",
    "with open(rec_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(marketing_recommendations, f, indent=2)\n",
    "print('Wrote marketing recommendations to', rec_path)\n",
    "\n",
    "# Create a short markdown report summarizing segments\n",
    "report_lines = []\n",
    "report_lines.append('# Customer Segmentation Report')\n",
    "report_lines.append(f'Generated: {datetime.utcnow().isoformat()} UTC')\n",
    "report_lines.append('\\n## Segment summaries')\n",
    "for label, p in profiles.items():\n",
    "    rpt = marketing_recommendations[label]\n",
    "    report_lines.append(f\"\\n### Segment {label} â€” size: {p['size']}\")\n",
    "    report_lines.append(f\"Top-3 features: {', '.join(p['top3_features'])}\")\n",
    "    report_lines.append(f\"Mean metrics: \")\n",
    "    for feat, val in p['means'].items():\n",
    "        report_lines.append(f\"- {feat}: {val:.2f}\")\n",
    "    report_lines.append(f\"Recommended channel: {rpt['channel']}\")\n",
    "    report_lines.append(f\"Suggested message: {rpt['message']}\")\n",
    "    report_lines.append(f\"Suggested offer: {rpt['offer']}\")\n",
    "    report_lines.append(f\"Baseline conv: {rpt['baseline_conversion']:.3f}, expected conv: {rpt['expected_conversion']:.3f} (abs lift {rpt['expected_absolute_lift']:.3f})\")\n",
    "\n",
    "report_path = os.path.join(os.path.dirname(DATA_PATH), 'segment_report.md')\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "print('Wrote human-readable report to', report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 18 - Save cluster profiles (JSON) and final sanity checks\n",
    "profiles_path = os.path.join(os.path.dirname(DATA_PATH), 'cluster_profiles.json')\n",
    "with open(profiles_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(profiles, f, indent=2)\n",
    "print('Saved cluster profile summary to', profiles_path)\n",
    "\n",
    "# Final sanity: ensure artifact files exist\n",
    "for p in [out_path, rec_path, report_path, profiles_path, os.path.join(os.path.dirname(DATA_PATH),'manifest.json')]:\n",
    "    print(p, 'exists?', os.path.exists(p))\n",
    "\n",
    "print('\\nAll notebook steps complete. Next: review the generated files, run targeted A/B tests for top segments, and iterate on features.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
